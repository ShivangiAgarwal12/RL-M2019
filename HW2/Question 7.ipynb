{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import numpy as np\n",
    "import pdb\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Helper function. poisson function\n",
    "prob_space = {}\n",
    "def poisson_prob(x,lam):\n",
    "    # x - event occurring.\n",
    "    # lam - expected value of the event.\n",
    "    #pdb.set_trace()\n",
    "    key = tuple(list((x,lam)))\n",
    "    if key not in prob_space.keys():\n",
    "        prob_space[key] = np.exp(-lam)*lam**x/np.math.factorial(x)\n",
    "    return prob_space[key]\n",
    "\n",
    "action_choice = np.arange(-5,6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%initialisations\n",
    "class car_rental():\n",
    "    def __init__(self, location_init, action_max):\n",
    "        self.location = location_init\n",
    "        self.action_space  = np.arange(action_max)\n",
    "        self.num_state = len(location_init)\n",
    "        self.max_req = 11\n",
    "        self.max_ret = 11\n",
    "      \n",
    "        \n",
    "    def reward(self, action, state_dict):\n",
    "#        pdb.set_trace()\n",
    "        tot_reward = 0\n",
    "        for req_A in range(self.max_req):\n",
    "            for req_B in range(self.max_req):\n",
    "                # Joint probability of request.\n",
    "                prob_req = poisson_prob(req_A,3)*poisson_prob(req_B,4)\n",
    "                # Number of cars update.\n",
    "                car_loc_A = min(req_A, self.location[0])\n",
    "                car_loc_B = min(req_B, self.location[1])\n",
    "                # Current reward.\n",
    "                reward = 10*(car_loc_A + car_loc_B)\n",
    "                \n",
    "                car_loc_A = min(max(self.location[0] - car_loc_A - action,0),20)\n",
    "                car_loc_B = min(max(self.location[1] - car_loc_B - action,0),20)\n",
    "                \n",
    "                reward = reward + (-2*np.abs(action))\n",
    "                \n",
    "                for ret_A in range(self.max_ret):\n",
    "                    for ret_B in range(self.max_ret):\n",
    "                        # Joint probability of return.\n",
    "                        prob_ret = poisson_prob(ret_A,2)*poisson_prob(ret_B,3)\n",
    "                        \n",
    "                        prob_joint = prob_ret*prob_req\n",
    "                        \n",
    "                        car_loc_A = min(car_loc_A + ret_A,20)\n",
    "                        car_loc_B = min(car_loc_B + ret_B,20)\n",
    "                        \n",
    "                        reward = prob_joint*(reward + \\\n",
    "                                             gamma*(state_dict[tuple(list((car_loc_A, car_loc_B)))]))\n",
    "                        tot_reward = tot_reward + reward\n",
    "        return tot_reward\n",
    "        \n",
    "    def ss_policy(self, action_all, state_dict):\n",
    "#        pdb.set_trace()\n",
    "        reward_action = []\n",
    "        for i in action_all:\n",
    "            tot_reward = 0\n",
    "            prob_store = []\n",
    "            for req_A in range(self.max_req):\n",
    "                for req_B in range(self.max_req):\n",
    "                    # Joint probability of request.\n",
    "                    prob_req = poisson_prob(req_A,3)*poisson_prob(req_B,4)\n",
    "                    # Number of cars update.\n",
    "                    car_loc_A = min(req_A, self.location[0])\n",
    "                    car_loc_B = min(req_B, self.location[1])\n",
    "                    # Current reward.\n",
    "                    reward = 10*(car_loc_A + car_loc_B)\n",
    "                    \n",
    "                    car_loc_A = min(max(self.location[0] - car_loc_A - i,0),20)\n",
    "                    car_loc_B = min(max(self.location[1] - car_loc_B - i,0),20)\n",
    "                    \n",
    "                    reward = reward + (-2*np.abs(i))\n",
    "                    \n",
    "                    for ret_A in range(self.max_ret):\n",
    "                        for ret_B in range(self.max_ret):\n",
    "                            # Joint probability of return.\n",
    "                            prob_ret = poisson_prob(ret_A,2)*poisson_prob(ret_B,3)\n",
    "                            \n",
    "                            prob_joint = prob_ret*prob_req\n",
    "                            \n",
    "                            car_loc_A = min(car_loc_A + ret_A,20)\n",
    "                            car_loc_B = min(car_loc_B + ret_B,20)\n",
    "                            prob_store.append(prob_joint)\n",
    "                            reward = prob_joint*(reward + \\\n",
    "                                                 gamma*(state_dict[tuple(list((car_loc_A, car_loc_B)))]))\n",
    "                            tot_reward = tot_reward + reward\n",
    "            reward_action.append(tot_reward)\n",
    "        return action_all[np.argmax(reward_action)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "Policy count =  1\n"
     ]
    }
   ],
   "source": [
    "#%% Poliy improvement and policy evaluation\n",
    "# Possible action choice.\n",
    "action_all = np.arange(-5,6)\n",
    "\n",
    "# Discount factor.\n",
    "gamma =0.9\n",
    "\n",
    "# Error bound.\n",
    "theta = 0.01\n",
    "\n",
    "# Exit condition for loop.\n",
    "loop = True\n",
    "\n",
    "# Local counter.\n",
    "cnt = 0\n",
    "\n",
    "# Initialize all the states.\n",
    "init_state = list(itertools.product(np.arange(21), repeat=2))\n",
    "\n",
    "# Empty dictionary.\n",
    "state = {}\n",
    "\n",
    "for i in range(len(init_state)):\n",
    "    state[init_state[i]] = 20\n",
    "    \n",
    "# Empty dictionary.\n",
    "policy = {}\n",
    "\n",
    "for i in range(len(init_state)):\n",
    "    policy[init_state[i]] = 0\n",
    "\n",
    "# Policy loop\n",
    "policy_stable = False\n",
    "    \n",
    "# Policy counter \n",
    "plc_cnt = 0\n",
    "\n",
    "while policy_stable== False:\n",
    "    while loop == True:\n",
    "        delta = 0.0\n",
    "        \n",
    "        for i in range(len(init_state)):\n",
    "            \n",
    "            action_choice = policy[init_state[i]]\n",
    "            \n",
    "            v = state[init_state[i]]\n",
    "        \n",
    "            agent_car = car_rental(init_state[i],20) #initial state\n",
    "            \n",
    "    #        pdb.set_trace()\n",
    "            value_next_state = agent_car.reward(action_choice, state) \n",
    "            delta = max(delta, np.abs((v - value_next_state)))\n",
    "            # print(delta)\n",
    "            state[init_state[i]] = value_next_state\n",
    "           \n",
    "        cnt = cnt + 1\n",
    "        print(cnt)\n",
    "        if delta < theta:\n",
    "            loop = False\n",
    "        \n",
    "    for i in range(len(init_state)):\n",
    "        old_action = policy[init_state[i]]\n",
    "        \n",
    "        agent_car = car_rental(init_state[i],20) #initial state    \n",
    "#        pdb.set_trace()\n",
    "        pi_policy = agent_car.ss_policy(action_all, state)\n",
    "        \n",
    "        if old_action == pi_policy:\n",
    "            policy_stable = True\n",
    "        else:\n",
    "            loop == True\n",
    "            policy[init_state[i]] = pi_policy\n",
    "        \n",
    "    plc_cnt += 1\n",
    "    print('Policy count = ', plc_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
