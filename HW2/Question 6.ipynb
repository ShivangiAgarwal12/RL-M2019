{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import numpy as np\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Initialisations\n",
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.row = 4\n",
    "        self.column = 4\n",
    "\n",
    "        # States in the grid world\n",
    "        self.state1 = np.empty((self.row,self.column))\n",
    "\n",
    "        # Value for each state\n",
    "        self.state_value = {}\n",
    "        value = 1\n",
    "        for i in range(self.row):\n",
    "            for j in range(self.column):\n",
    "                self.state1[i,j] = value\n",
    "                self.state_value[value] = 0\n",
    "                value +=1\n",
    "\n",
    "        # Action space\n",
    "        self.action = {6: 'left', 7: 'right', 8: 'north', 9: 'south'}\n",
    "        \n",
    "        # Porbability of picking each action.\n",
    "#        pi_act\n",
    "        \n",
    "        # Action space (numerical).\n",
    "        act_n = np.arange(6,10)\n",
    "        \n",
    "        # Policy mapping. \n",
    "        self.act_s = {}\n",
    "        for i in range(self.row):\n",
    "            for j in range(self.column):\n",
    "                self.act_s[self.state1[i,j]] = np.random.choice(act_n)\n",
    "\n",
    "    # State transition and reward.\n",
    "    def action_state(self,action,state):\n",
    "        reward = 0\n",
    "        state1 = np.inf\n",
    "        loop = True\n",
    "        #leftmost corner condition\n",
    "        if state in (self.state1[3,0],self.state1[1,0],self.state1[2,0]):\n",
    "            if action == self.action[6]:  #left\n",
    "                reward = -1\n",
    "                state1 = state\n",
    "            if action == self.action[7]:  #right\n",
    "                reward = -1\n",
    "                state1 = state + 1\n",
    "            if action == self.action[8]: #north\n",
    "                state1 = state - 4\n",
    "                if state1 == self.state1[0,0]:\n",
    "                    reward = 0\n",
    "                    state1 = 1\n",
    "                else:\n",
    "                    reward  = -1\n",
    "                    state1 = state - 4\n",
    "            if action == self.action[9]: #south\n",
    "                state1 = state + 4\n",
    "                if state1 not in self.state1:\n",
    "                    reward = -1\n",
    "                    state1 = state\n",
    "                else:\n",
    "                    reward = -1\n",
    "                    state1 = state + 4\n",
    "            loop = False\n",
    "\n",
    "        #right most condition\n",
    "        if state == self.state1[0,3] or state == self.state1[1,3] or state == self.state1[2,3]:\n",
    "            if action == self.action[6]: #left\n",
    "                reward = -1\n",
    "                state1 = state - 1\n",
    "            if action == self.action[7]: #right\n",
    "                reward = -1\n",
    "                state1 = state\n",
    "            if action == self.action[8]: # north\n",
    "                state1 = state - 4\n",
    "                if state1 not in self.state1:\n",
    "                    reward = -1\n",
    "                    state1 = state\n",
    "                else:\n",
    "                    reward = -1\n",
    "                    state1 = state - 4\n",
    "            if action == self.action[9]: #south\n",
    "                state1 = state + 4\n",
    "                if state1 == self.state1[3,3]:\n",
    "                    reward = 0\n",
    "                    state1 = 16\n",
    "                else:\n",
    "                    reward = -1\n",
    "                    state1 = state + 4\n",
    "            loop = False\n",
    "\n",
    "\n",
    "        elif loop == True:\n",
    "            if action == self.action[6]: #left\n",
    "                state1 = state - 1\n",
    "                if state1 ==  self.state1[0,0]:\n",
    "                    reward = 0\n",
    "                    state1 = 1\n",
    "                else:\n",
    "                    reward = -1\n",
    "                    state1 = state - 1\n",
    "\n",
    "            if action == self.action[7]: #right\n",
    "                state1 = state + 1\n",
    "                if state1 == self.state1[3,3]:\n",
    "                    reward = 0\n",
    "                    state1 = 16\n",
    "                else:\n",
    "                    reward = -1\n",
    "                    state1 = state + 1\n",
    "\n",
    "            if action == self.action[8]: #north\n",
    "                state1 = state - 4\n",
    "                if state1 not in self.state1:\n",
    "                    reward = -1\n",
    "                    state1 = state\n",
    "                else:\n",
    "                    reward = -1\n",
    "                    state1 = state - 4\n",
    "\n",
    "            if action == self.action[9]: #south\n",
    "                state1 = state + 4\n",
    "                if state1 not in self.state1:\n",
    "                    reward = -1\n",
    "                    state1 = state\n",
    "                else:\n",
    "                    reward = -1\n",
    "                    state1 = state + 4\n",
    "        return state1, reward\n",
    "\n",
    "\n",
    "  #choosing any action with equal probability\n",
    "    def policy(self,state):\n",
    "        ac = np.arange(6,10)\n",
    "        action2 = []\n",
    "        for j in range(len(ac)):\n",
    "            action1 = self.action[ac[j]]\n",
    "            action2.append(action1)\n",
    "        action = np.random.choice(list(self.action.keys()))\n",
    "        return action           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#%% Policy iteration\n",
    "# Initialize the agent.\n",
    "a = Agent()\n",
    "\n",
    "# Total number of states\n",
    "num_states = a.row*a.column\n",
    "\n",
    "# Total number of actions\n",
    "action_space = a.action.keys()\n",
    "num_actions = len(action_space)\n",
    "\n",
    "# Numerical action space.\n",
    "act_space = np.arange(6,10)\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# Convergence variable\n",
    "theta = 0.0001\n",
    "\n",
    "# Counter just to keep track\n",
    "count = 0\n",
    "\n",
    "# Loop termination condition\n",
    "loop1  = True\n",
    "\n",
    "#temp values for each state\n",
    "ac_values = np.zeros((num_states,num_actions))\n",
    "\n",
    "# Policy loop\n",
    "policy_stable = False\n",
    "\n",
    "# Policy counter\n",
    "plc_cnt = 0\n",
    "\n",
    "# Optimal policy\n",
    "pol = [\"\" for x in range(num_states)]\n",
    "action3 = [\"\" for x in range(num_actions)]\n",
    "\n",
    "#############\n",
    "# Algorithm\n",
    "#############\n",
    "\n",
    "while policy_stable == False:\n",
    "    #pdb.set_trace()\n",
    "    \n",
    "    \n",
    "    while loop1 == True:\n",
    "        #policy evaluation\n",
    "        # Initialize delta to 0.\n",
    "        delta = 0.0\n",
    "        \n",
    "        # Value convergence loop.\n",
    "        val_con = 0\n",
    "        # Loop over 2 - 15 (1 and 16 are terminal states).\n",
    "        for i in range(2,num_states):\n",
    "            # State value of a particular state\n",
    "            v = np.copy(a.state_value[i])\n",
    "    \n",
    "            # Action for the state i.\n",
    "            cur_act = a.action[a.act_s[i]]\n",
    "\n",
    "            [state_transit,rew] = a.action_state(cur_act, i)\n",
    "\n",
    "            # Discounted updates\n",
    "            temp = 1.0*(rew + gamma*np.copy(a.state_value[state_transit]))\n",
    "            \n",
    "            a.state_value[i] = np.copy(temp)\n",
    "        \n",
    "            # v-V(s), compare for a particular state\n",
    "            final = temp - v\n",
    "            delta = max(delta, abs(final))\n",
    "\n",
    "            # Policy evaluation convergence check.\n",
    "            if delta<theta:\n",
    "                val_con = val_con + 1\n",
    "                loop1 = False\n",
    "        \n",
    "#        if val_con == 14:\n",
    "#            #pdb.set_trace()\n",
    "#            loop1 = False\n",
    "       \n",
    "    # Policy convergence check.\n",
    "    pol_con = 0\n",
    "    # Policy improvement starts\n",
    "    for i in range(2,num_states):\n",
    "        new_action = []\n",
    "        # Old action.\n",
    "        old_action = a.act_s[i]\n",
    "        \n",
    "        # State action pair.\n",
    "        temp_s_a = np.zeros((len(act_space),1))\n",
    "        \n",
    "        for j in range(len(act_space)):\n",
    "            [state_transit,rew] = a.action_state(a.action[act_space[j]], i)\n",
    "            temp_s_a[j,0] = rew + gamma*a.state_value[state_transit]\n",
    "        \n",
    "        # New policy.\n",
    "        #pdb.set_trace()\n",
    "        pi_cur = np.argmax(temp_s_a) + 6\n",
    "        \n",
    "        if old_action != pi_cur:\n",
    "            a.act_s[i] = pi_cur\n",
    "        else:\n",
    "            pol_con = pol_con+1\n",
    "            \n",
    "    if pol_con == num_states-2:\n",
    "        policy_stable = True\n",
    "        loop1 = False\n",
    "         \n",
    "                \n",
    "    count = count +1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ||←||←||←|\n",
      "|↑||←||←||←|\n",
      "|↑||←||↓||↓|\n",
      "|←||→||→|| |\n"
     ]
    }
   ],
   "source": [
    "#%% Plot the policy\n",
    "        \n",
    "def plot_pol(act):\n",
    "    if act == 6:\n",
    "        temp = '|' + chr(8592) + '|'\n",
    "    elif act == 7:\n",
    "        temp = '|' +  chr(8594) + '|'\n",
    "    elif act == 8:\n",
    "        temp = '|' + chr(8593) + '|'\n",
    "    elif act == 9:\n",
    "        temp = '|' + chr(8595) + '|'\n",
    "        \n",
    "    return temp\n",
    "\n",
    "plt_pol = []\n",
    "plt_pol.append('| |')\n",
    "cnt = 0\n",
    "cnt_st = 0\n",
    "for i in range(2,num_states):\n",
    "    if np.mod(i,4)==0:\n",
    "        plt_pol[cnt] = plt_pol[cnt] + plot_pol(a.act_s[i])\n",
    "        cnt = cnt + 1\n",
    "        plt_pol.append('')\n",
    "    else:\n",
    "        plt_pol[cnt] = plt_pol[cnt] + plot_pol(a.act_s[i])\n",
    "plt_pol[cnt] = plt_pol[cnt] + ('| |')\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    print(plt_pol[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
