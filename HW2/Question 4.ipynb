{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Bellman Optimal Equations by solving non-linear equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import numpy as np\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%initialisations\n",
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.row = 5\n",
    "        self.column = 5\n",
    "\n",
    "        #states in the grid world\n",
    "        self.state1 = np.empty((self.row,self.column))\n",
    "\n",
    "        #value for each state\n",
    "        self.state_value = {}\n",
    "        value = 1\n",
    "        for i in range(self.row):\n",
    "            for j in range(self.column):\n",
    "                self.state1[i,j] = value\n",
    "                self.state_value[value] = None\n",
    "                value +=1\n",
    "\n",
    "        #action space\n",
    "        self.action = {6: 'left', 7: 'right', 8: 'north', 9: 'south'}\n",
    "\n",
    "    def action_state(self,action,state):\n",
    "#        pdb.set_trace()\n",
    "        reward = 0\n",
    "        tot_rew = 0.0\n",
    "        state1 = np.inf\n",
    "        loop = True\n",
    "        #leftmost corner condition\n",
    "        if state in (self.state1[0,0],self.state1[4,0],\\\n",
    "                     self.state1[1,0],self.state1[2,0],\\\n",
    "                     self.state1[3,0]):\n",
    "            if action == self.action[6]:\n",
    "                reward = -1\n",
    "                state1 = state\n",
    "            if action == self.action[7]:\n",
    "                reward = 0\n",
    "                state1 = state + 1\n",
    "            if action == self.action[8]:\n",
    "                state1 = state - 5\n",
    "                if state1 not in self.state1:\n",
    "                    reward = -1\n",
    "                    state1 = state\n",
    "                else:\n",
    "                    reward  = 0\n",
    "                    state1 = state - 5\n",
    "            if action == self.action[9]:\n",
    "                state1 = state + 5\n",
    "                if state1 not in self.state1:\n",
    "                    reward = -1\n",
    "                    state1 = state\n",
    "                else:\n",
    "                    reward = 0\n",
    "                    state1 = state + 5\n",
    "            tot_rew += reward\n",
    "            loop = False\n",
    "\n",
    "        #right most condition\n",
    "        if state == self.state1[0,4] or\\\n",
    "        state == self.state1[1,4] or \\\n",
    "        state == self.state1[2,4] or \\\n",
    "        state == self.state1[3,4] or \\\n",
    "        state == self.state1[4,4]:\n",
    "            if action == self.action[6]:\n",
    "                reward = 0\n",
    "                state1 = state - 1\n",
    "            if action == self.action[7]:\n",
    "                reward = -1\n",
    "                state1 = state\n",
    "            if action == self.action[8]:\n",
    "                state1 = state - 5\n",
    "                if state1 not in self.state1:\n",
    "                    reward = -1\n",
    "                    state1 = state\n",
    "                else:\n",
    "                    reward = 0\n",
    "                    state1 = state - 5\n",
    "            if action == self.action[9]:\n",
    "                state1 = state + 5\n",
    "                if state1 not in self.state1:\n",
    "                    reward = -1\n",
    "                    state1 = state\n",
    "                else:\n",
    "                    reward = 0\n",
    "                    state1 = state + 5\n",
    "            tot_rew += reward\n",
    "            loop = False\n",
    "\n",
    "\n",
    "        #best conditions\n",
    "        if state == self.state1[0,1]:\n",
    "            action = self.action\n",
    "            reward = 10\n",
    "            state1 = state + 20\n",
    "            loop = False\n",
    "            tot_rew = reward\n",
    "\n",
    "        if state  == self.state1[0,3]:\n",
    "            action = self.action\n",
    "            reward = 5\n",
    "            state1 = state + 10\n",
    "            tot_rew = reward\n",
    "            loop = False\n",
    "\n",
    "        elif loop == True:\n",
    "            if action == self.action[6]:\n",
    "                state1 = state - 1\n",
    "                reward = 0\n",
    "            if action == self.action[7]:\n",
    "                state1 = state + 1\n",
    "                reward = 0\n",
    "            if action == self.action[8]:\n",
    "                state1 = state - 5\n",
    "                if state1 not in self.state1:\n",
    "                    reward = -1\n",
    "                    state1 = state\n",
    "                else:\n",
    "                    reward = 0\n",
    "                    state1 = state - 5\n",
    "            if action == self.action[9]:\n",
    "                state1 = state + 5\n",
    "                if state1 not in self.state1:\n",
    "                    reward = -1\n",
    "                    state1 = state\n",
    "                else:\n",
    "                    reward = 0\n",
    "                    state1 = state + 5\n",
    "            tot_rew += reward\n",
    "        return state1, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import itertools\n",
    "\n",
    "# Get all permutations of [1, 2, 3]\n",
    "x = [0, 0.25, 1/3, 0.5, 2/3, 0.75, 1]\n",
    "\n",
    "perm = [p for p in itertools.product(x, repeat=4)]\n",
    "\n",
    "# Print the obtained permutations\n",
    "temp = []\n",
    "for i in list(perm):\n",
    "    temp.append(i)\n",
    "\n",
    "# Possible policy for each state.\n",
    "mu_pi = []\n",
    "for i in range(len(temp)):\n",
    "    if np.sum(temp[i])==1:\n",
    "        mu_pi.append(temp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hold\n"
     ]
    }
   ],
   "source": [
    "#%% Possible combination.\n",
    "x_agent = Agent()\n",
    "\n",
    "# Number of states.\n",
    "num_states = x_agent.row*x_agent.column\n",
    "\n",
    "# Number of actions and possible actions.\n",
    "action_space = x_agent.action.keys()\n",
    "num_actions = len(action_space)\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "V_s = np.zeros((num_states,len(mu_pi)))\n",
    "\n",
    "# Action map\n",
    "def act_map(mu):\n",
    "    act_n = []\n",
    "\n",
    "    act_space = np.arange(6,10)\n",
    "\n",
    "    prob_act = []\n",
    "    for i in range(len(mu)):\n",
    "        if mu[i] != 0:\n",
    "            act_n.append(act_space[i])\n",
    "            prob_act.append(mu[i])\n",
    "\n",
    "    return [prob_act,np.array(act_n)]\n",
    "\n",
    "for pol in range(len(mu_pi)):\n",
    "    if pol == 22:\n",
    "        print('hold')\n",
    "#        pdb.set_trace()\n",
    "    #pdb.set_trace()\n",
    "    [prob_act,ac] = act_map(mu_pi[pol])\n",
    "\n",
    "    num_actions = len(ac)\n",
    "\n",
    "    # States transited for each state.\n",
    "    state_transit = np.zeros((num_states, num_actions))\n",
    "    rew = np.zeros((num_states, num_actions))\n",
    "\n",
    "    # Probability of transition.\n",
    "    P_pi = np.zeros((num_states,num_states))\n",
    "\n",
    "    update = 0\n",
    "    for r in range(x_agent.row):\n",
    "        for c in range(x_agent.column):\n",
    "            # Prob of action.\n",
    "\n",
    "            for l in range(len(ac)):\n",
    "                [state_transit[update,l],rew[update,l]] =\\\n",
    "                x_agent.action_state(x_agent.action[ac[l]], x_agent.state1[r,c])\n",
    "\n",
    "                # State\n",
    "                s_temp = int(state_transit[update,l]) -1\n",
    "                P_pi[update,s_temp] = P_pi[update,s_temp] + prob_act[l]\n",
    "            update+=1\n",
    "\n",
    "    #pdb.set_trace()\n",
    "    #finding the reward\n",
    "    reward = np.zeros((num_states,1))\n",
    "\n",
    "    for i in range(num_states):\n",
    "        for j in range(len(ac)):\n",
    "            reward[i] = reward[i] + prob_act[j]*rew[i,j]\n",
    "\n",
    "    # Finding value function\n",
    "    I = np.identity(num_states)\n",
    "    m = np.linalg.inv(np.eye(num_states) - gamma*P_pi)\n",
    "    # Optimal value of the states.\n",
    "    v_pi = np.matmul(m,reward)\n",
    "    V_s[0:,pol] = np.reshape(v_pi,(num_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Optimal Policy.\n",
    "\n",
    "pi_s = np.zeros((4,num_states))\n",
    "v_final = np.zeros((num_states,1))\n",
    "for i in range(num_states):\n",
    "    v_final[i] = np.max(V_s[i,:])\n",
    "    pi_s[:,i] = mu_pi[np.argmax(V_s[i,:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.35956481, 24.4194281 ,  8.68016838, 18.4501845 ,  5.25049028,\n",
       "         6.45182012, 21.97748529,  7.59868837, 16.60516605,  4.63946605,\n",
       "         5.38602382, 19.77973676,  6.1725332 , 14.94464945,  4.32719411,\n",
       "         4.28716895, 17.80176308,  4.88740847, 13.4501845 ,  3.77732005,\n",
       "         3.31337273, 16.02158677,  3.82844646, 12.10516605,  3.1324146 ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_final.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
