{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear equations for solving $v_\\pi(s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import numpy as np\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%initialisations\n",
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.row = 5\n",
    "        self.column = 5\n",
    "\n",
    "        #states in the grid world\n",
    "        self.state1 = np.empty((self.row,self.column))\n",
    "\n",
    "        #value for each state\n",
    "        self.state_value = {}\n",
    "        value = 1\n",
    "        for i in range(self.row):\n",
    "            for j in range(self.column):\n",
    "                self.state1[i,j] = value\n",
    "                self.state_value[value] = None\n",
    "                value +=1\n",
    "\n",
    "        #action space\n",
    "        self.action = {6: 'left', 7: 'right', 8: 'north', 9: 'south'}\n",
    "\n",
    "    def action_state(self,action,state):\n",
    "#        pdb.set_trace()\n",
    "        reward = 0\n",
    "        tot_rew = 0.0\n",
    "        state1 = np.inf\n",
    "        loop = True\n",
    "        #leftmost corner condition\n",
    "        if state in (self.state1[0,0],self.state1[4,0],\\\n",
    "                     self.state1[1,0],self.state1[2,0],\\\n",
    "                     self.state1[3,0]):\n",
    "            if action == self.action[6]:\n",
    "                reward = -1\n",
    "                state1 = state\n",
    "            if action == self.action[7]:\n",
    "                reward = 0\n",
    "                state1 = state + 1\n",
    "            if action == self.action[8]:\n",
    "                state1 = state - 5\n",
    "                if state1 not in self.state1:\n",
    "                    reward = -1\n",
    "                    state1 = state\n",
    "                else:\n",
    "                    reward  = 0\n",
    "                    state1 = state - 5\n",
    "            if action == self.action[9]:\n",
    "                state1 = state + 5\n",
    "                if state1 not in self.state1:\n",
    "                    reward = -1\n",
    "                    state1 = state\n",
    "                else:\n",
    "                    reward = 0\n",
    "                    state1 = state + 5\n",
    "            tot_rew += reward\n",
    "            loop = False\n",
    "\n",
    "        #right most condition\n",
    "        if state == self.state1[0,4] or\\\n",
    "        state == self.state1[1,4] or \\\n",
    "        state == self.state1[2,4] or \\\n",
    "        state == self.state1[3,4] or \\\n",
    "        state == self.state1[4,4]:\n",
    "            if action == self.action[6]:\n",
    "                reward = 0\n",
    "                state1 = state - 1\n",
    "            if action == self.action[7]:\n",
    "                reward = -1\n",
    "                state1 = state\n",
    "            if action == self.action[8]:\n",
    "                state1 = state - 5\n",
    "                if state1 not in self.state1:\n",
    "                    reward = -1\n",
    "                    state1 = state\n",
    "                else:\n",
    "                    reward = 0\n",
    "                    state1 = state - 5\n",
    "            if action == self.action[9]:\n",
    "                state1 = state + 5\n",
    "                if state1 not in self.state1:\n",
    "                    reward = -1\n",
    "                    state1 = state\n",
    "                else:\n",
    "                    reward = 0\n",
    "                    state1 = state + 5\n",
    "            tot_rew += reward\n",
    "            loop = False\n",
    "\n",
    "\n",
    "        #best conditions\n",
    "        if state == self.state1[0,1]:\n",
    "            action = self.action\n",
    "            reward = 10\n",
    "            state1 = state + 20\n",
    "            loop = False\n",
    "            tot_rew = reward\n",
    "\n",
    "        if state  == self.state1[0,3]:\n",
    "            action = self.action\n",
    "            reward = 5\n",
    "            state1 = state + 15\n",
    "            tot_rew = reward\n",
    "            loop = False\n",
    "\n",
    "        elif loop == True:\n",
    "            if action == self.action[6]:\n",
    "                state1 = state - 1\n",
    "                reward = 0\n",
    "            if action == self.action[7]:\n",
    "                state1 = state + 1\n",
    "                reward = 0\n",
    "            if action == self.action[8]:\n",
    "                state1 = state - 5\n",
    "                if state1 not in self.state1:\n",
    "                    reward = -1\n",
    "                    state1 = state\n",
    "                else:\n",
    "                    reward = 0\n",
    "                    state1 = state - 5\n",
    "            if action == self.action[9]:\n",
    "                state1 = state + 5\n",
    "                if state1 not in self.state1:\n",
    "                    reward = -1\n",
    "                    state1 = state\n",
    "                else:\n",
    "                    reward = 0\n",
    "                    state1 = state + 5\n",
    "            tot_rew += reward\n",
    "        return state1, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%initialising policy evaluation\n",
    "x_agent = Agent()\n",
    "counter = 0\n",
    "for i in range(x_agent.row):\n",
    "    for j in range(x_agent.column):\n",
    "        counter +=1\n",
    "# Number of states.\n",
    "num_states = counter\n",
    "a= x_agent.row*x_agent.column\n",
    "\n",
    "# Number of actions and possible actions.\n",
    "action_space = x_agent.action.keys()\n",
    "num_actions = len(action_space)\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "\n",
    "    \n",
    "temp = 0\n",
    "prob = np.asarray((x_agent.row,x_agent.column))\n",
    "prob = np.zeros((prob))\n",
    "state_transit = np.zeros((x_agent.row*x_agent.column,num_actions))\n",
    "rew = np.zeros((x_agent.row*x_agent.column,num_actions))\n",
    "update = 0\n",
    "for r in range(x_agent.row):\n",
    "    for c in range(x_agent.column):\n",
    "        ac = np.arange(6,10)\n",
    "        for l in range(len(ac)):\n",
    "            [state_transit[update,l],rew[update,l]] =\\\n",
    "            x_agent.action_state(x_agent.action[ac[l]], x_agent.state1[r,c])\n",
    "        update+=1\n",
    "#finding the probability of being in a state        \n",
    "prob1 = np.zeros((a,a)) #finding p(pi)\n",
    "prob = np.zeros((a,a))  \n",
    "     \n",
    "for p in range(a):\n",
    "    p_v = 1 \n",
    "    for q in range(a):\n",
    "        prob[p,q] = p_v\n",
    "        p_v +=1\n",
    "for row in range(a):\n",
    "    for column in range(a):\n",
    "        for k in range(num_actions):\n",
    "            if prob[row,column] == state_transit[row,k]:\n",
    "                prob1[row,column] += 0.25\n",
    "\n",
    "#finding the reward\n",
    "rew1 = np.zeros((a,1))\n",
    "for i in range(a):\n",
    "    rew1[i] = 0.25*np.sum(rew[i])\n",
    "\n",
    "#finding value function\n",
    "I = np.identity(25)\n",
    "value = np.zeros((25))\n",
    "m = np.linalg.inv(I - gamma*prob1)\n",
    "n = np.matmul(m,rew1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$v_\\pi(s) = (I-$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.282,  8.762,  4.097,  4.403,  0.998,  1.483,  2.922,  2.058,\n",
       "         1.558,  0.258,  0.015,  0.682,  0.569,  0.204, -0.556, -1.002,\n",
       "        -0.475, -0.414, -0.663, -1.266, -1.882, -1.375, -1.27 , -1.473,\n",
       "        -2.029]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(n.T,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
