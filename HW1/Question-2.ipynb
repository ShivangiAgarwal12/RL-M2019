{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate graph 2.3, solve exercise 2.6 for stationary cases ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%Initialisations\n",
    "#total number of bandit arms\n",
    "n = 10\n",
    "\n",
    "#total number of experiments\n",
    "t = 1000\n",
    "\n",
    "#probability of random exploration(fraction)\n",
    "epsilon = []\n",
    "epsilon = [0,0.1]\n",
    "\n",
    "#number of bandits(episodes)\n",
    "bandit_number = 2000\n",
    "\n",
    "#current value estimate of action\n",
    "Q = np.zeros((bandit_number,n))\n",
    "init_value = 5.0\n",
    "Q_opt = np.full((bandit_number,n),5.0)\n",
    "Q_greedy = np.zeros((bandit_number,n))\n",
    "Q_star_s = np.random.normal(loc = 0, scale = 1, size = (bandit_number,n) )\n",
    "\n",
    "\n",
    "#number of times an action was chosen for each bandit\n",
    "alpha = 0.1\n",
    "\n",
    "#optimal action that should be chosen for each bandit\n",
    "a_opt = np.zeros((bandit_number,n)) \n",
    "\n",
    "#cumulative reward at each episode for chossing an action by probability epsilon\n",
    "avg_reward = np.asmatrix(np.zeros((len(epsilon),t)))\n",
    "\n",
    "opt_act = np.asmatrix(np.zeros((len(epsilon),t)))\n",
    "opt_act_s = np.asmatrix(np.zeros((len(epsilon),t)))\n",
    "\n",
    "#function Bandit taking action and bandit as input, returning reward.\n",
    "\n",
    "def bandit(a,ids):\n",
    "    R = np.random.normal(loc=Q_star[ids,a],scale=1,size=1)\n",
    "    return R\n",
    "\n",
    "def bandit_s(a,ids):\n",
    "    R_s = np.random.normal(loc = Q_star_s[ids,a], scale = 1, size = 1)\n",
    "    return R_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%A simple bandit algorithm\n",
    "for k in range(len(epsilon)):\n",
    "    if (epsilon[k] == 0):\n",
    "        Q = Q_opt.copy()\n",
    "    else :\n",
    "        Q = Q_greedy.copy()\n",
    "\n",
    "    #Q = np.zeros((bandit_number,n))\n",
    "    init_value = np.random.normal(loc = 0, scale = 1)\n",
    "    Q_star = np.asarray([[init_value for column in range(n)] for row in range(bandit_number)])\n",
    "    \n",
    "\n",
    "    for episode in range(1, t):\n",
    "        local_reward = 0.0\n",
    "        local_reward_s = 0.0\n",
    "        action_history = 0.0\n",
    "        action_history_s = 0.0\n",
    "        upper_rewards = 0.0\n",
    "        for machine in range(1,bandit_number):\n",
    "            p = np.random.rand()  \n",
    "            if p <= epsilon[k]:      \n",
    "                a =  np.random.randint(1,high=n)\n",
    "            else:\n",
    "                a = np.argmax(Q[machine,:])\n",
    "\n",
    "            #reward for choosing an action a for corresponding machine\n",
    "            #update number of times action was chosen\n",
    "            non_stationarity = False\n",
    "            for est in range(2):\n",
    "                if est == 0:\n",
    "                    non_stationarity == True\n",
    "                    rewards = bandit(a,machine)\n",
    "                    Q[machine,a] = Q[machine,a] + alpha*(rewards-Q[machine,a])\n",
    "                    #adding the local rewards\n",
    "                    local_reward += rewards\n",
    "                    #calculating an optimal action for each bandit\n",
    "                    idx = np.argmax(Q_star[machine,:])\n",
    "                    action_history += (a==idx)\n",
    "                else:\n",
    "                    rewards_s = bandit_s(a,machine)\n",
    "                    Q[machine,a] = Q[machine,a] + alpha*(rewards_s-Q[machine,a])\n",
    "                    local_reward_s += rewards_s\n",
    "                    idx = np.argmax(Q_star_s[machine,:])\n",
    "                    action_history_s += (a==idx)\n",
    "                    non_stationarity == False\n",
    "                    \n",
    "        #update Q_star value with a noise \n",
    "        if (non_stationarity == True):\n",
    "            Q_star += np.random.normal(loc= 0, scale = 0.01, size = (bandit_number,n))\n",
    "            \n",
    "    \n",
    "        avg_reward[k,episode] = (local_reward/bandit_number)\n",
    "        opt_act[k,episode]= (action_history/bandit_number)\n",
    "        opt_act_s[k,episode] = (action_history_s/bandit_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%plot graph of percentage of chossing optimal actions\n",
    "for reward1 in range(len(epsilon)):\n",
    "    if epsilon[reward1] == 0:\n",
    "        ax = plt.plot(100*opt_act[reward1,:].T,label='Q = 5 ' + '$\\epsilon$ = ' +str(epsilon[reward1]))\n",
    "    else:\n",
    "        ax = plt.plot(100*opt_act[reward1,:].T,label = 'Q = 0 ' + '$\\epsilon$ = ' + str(epsilon[reward1]))\n",
    "    plt.xlabel('Steps')\n",
    "    plt.gca().set_yticklabels(['{:.0f}%'.format(x) for x in plt.gca().get_yticks()])\n",
    "    plt.ylabel('Optimal Action(%)')\n",
    "    plt.title('Non-stationarity for optimistic initial value')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%plot graph for stationarity\n",
    "for reward2 in range(len(epsilon)):\n",
    "    if epsilon[reward2] == 0:\n",
    "        ax = plt.plot(100*opt_act_s[reward2,:].T,label = 'Q = 5 ' + '$\\epsilon$ = ' + str(epsilon[reward2]))\n",
    "    else:\n",
    "        ax = plt.plot(100*opt_act_s[reward2,:].T,label = 'Q = 0 ' + '$\\epsilon$ =' + str(epsilon[reward2]))\n",
    "    plt.xlabel('Steps')\n",
    "    plt.gca().set_yticklabels(['{:.0f}%'.format(x) for x in plt.gca().get_yticks()])\n",
    "    plt.title('Stationary optimal action values')\n",
    "    plt.ylabel('Optimal actions(%)')\n",
    "    plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
