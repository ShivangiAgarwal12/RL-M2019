{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%Initialisations\n",
    "#total number of bandit arms\n",
    "n = 10\n",
    "\n",
    "#total number of experiments\n",
    "t = 1000\n",
    "\n",
    "#probability of random exploration(fraction)\n",
    "epsilon = 0.1\n",
    "\n",
    "#number of bandits(episodes)\n",
    "bandit_number = 2000\n",
    "\n",
    "#current value estimate of action\n",
    "Q = np.zeros((bandit_number,n)) \n",
    "\n",
    "#number of times an action was chosen for each bandit\n",
    "increments = []\n",
    "N = np.zeros((bandit_number,n)) \n",
    "alpha = 0.1\n",
    "increments = [N,alpha]\n",
    "\n",
    "#optimal action that should be chosen for each bandit\n",
    "a_opt = np.zeros((bandit_number,n)) \n",
    "\n",
    "#initial same values to all the arms\n",
    "#init_value = np.random.normal(loc = 0, scale = 1)\n",
    "#Q_star = np.asarray([[init_value for column in range(n)] for row in range(bandit_number)])\n",
    "#Q_star = np.random.normal(loc=0,scale=1,size=(bandit_number,n))\n",
    "\n",
    "#cumulative reward at each episode for chossing an action by probability epsilon\n",
    "avg_reward = np.asmatrix(np.zeros((len(increments),t)))\n",
    "\n",
    "\n",
    "opt_act = np.asmatrix(np.zeros((len(increments),t)))\n",
    "\n",
    "#function Bandit taking action and bandit as input, returning reward.\n",
    "def bandit(a,ids):\n",
    "    R = np.random.normal(loc=Q_star[ids,a],scale=1,size=1)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%A simple bandit algorithm\n",
    "for k in range(len(increments)):\n",
    "    \n",
    "    Q = np.zeros((bandit_number,n))\n",
    "    increments[0] = np.zeros((bandit_number,n))\n",
    "    #N = np.zeros((bandit_number,n))\n",
    "    init_value = np.random.normal(loc = 0, scale = 1)\n",
    "    Q_star = np.asarray([[init_value for column in range(n)] for row in range(bandit_number)])\n",
    "\n",
    "    for episode in range(1, t):\n",
    "        local_reward = 0.0\n",
    "        action_history = 0.0\n",
    "        upper_rewards = 0.0\n",
    "        for machine in range(1,bandit_number):\n",
    "            p = np.random.rand()  \n",
    "            if p <= epsilon:      \n",
    "                a =  np.random.randint(1,high=n)\n",
    "            else:\n",
    "                a = np.argmax(Q[machine,:])\n",
    "\n",
    "            #reward for choosing an action a for corresponding machine\n",
    "            rewards = bandit(a,machine) \n",
    "            \n",
    "            #update number of times action was chosen\n",
    "            if (k == 0):\n",
    "                increments[k][machine,a] += 1\n",
    "                Q[machine,a] = Q[machine,a] + (1/increments[k][machine,a])*(rewards-Q[machine,a])\n",
    "            else :\n",
    "                Q[machine,a] = Q[machine,a] + (increments[k])*(rewards-Q[machine,a])\n",
    "                \n",
    "            #adding the local rewards\n",
    "            local_reward += rewards\n",
    "            \n",
    "            \n",
    "            #calculating an optimal action for each bandit\n",
    "            idx = np.argmax(Q_star[machine,:])\n",
    "            action_history += (a==idx)\n",
    "\n",
    "        #update Q_star value with a noise \n",
    "        Q_star += np.random.normal(loc= 0, scale = 0.01, size = (bandit_number,n))\n",
    "        \n",
    "\n",
    "        avg_reward[k,episode] = (local_reward/bandit_number)\n",
    "        opt_act[k,episode]= (action_history/bandit_number)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%plot graphs for average rewards\n",
    "for reward in range(len(increments)):\n",
    "    if reward == 0:\n",
    "        plt.plot(avg_reward[reward,:].T,label = 'Increment = 1/N')\n",
    "    else:\n",
    "        plt.plot(avg_reward[reward,:].T,label = 'alpha = 0.1')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Average Rewards')\n",
    "    plt.legend()\n",
    "    plt.title('Non-Stationarity for epsilon = 0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%plot graph of percentage of chossing optimal actions\n",
    "for reward1 in range(len(increments)):\n",
    "    if reward1 == 0:\n",
    "        ax = plt.plot(100*opt_act[reward1,:].T,label='Increments = 1/N')\n",
    "    else:\n",
    "        ax = plt.plot(100*opt_act[reward1,:].T,label = 'alpha = 0.1')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.gca().set_yticklabels(['{:.0f}%'.format(x) for x in plt.gca().get_yticks()])\n",
    "    plt.title(' for Epsilon = 0.1')\n",
    "    plt.ylabel('Optimal Action(%)')\n",
    "    plt.title('Non-stationarity for epsilon = 0.1')\n",
    "    plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
